{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sec_edgar_downloader import Downloader\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from lxml import etree\n",
    "import io\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import openai\n",
    "import requests\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the complete submission data from the SEC-EDGAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# My email address for making requests to the SEC\n",
    "email = \"shradhagodse125@gmail.com\"\n",
    "\n",
    "# Initialize a downloader instance with my email address\n",
    "dl = Downloader(os.path.join(os.getcwd(), \"sec_filings\"), email)\n",
    "\n",
    "# List of company tickers, I selected Apple, Microsoft and Google\n",
    "tickers = [\"AAPL\", \"MSFT\", \"GOOGL\"] \n",
    "\n",
    "# Function to download 10-K filings\n",
    "def download_10k(tickers, start_year=1995, end_year=2023):\n",
    "    for ticker in tickers:\n",
    "        for year in range(start_year, end_year + 1):\n",
    "            # Download 10-K forms for the specified year\n",
    "            dl.get(\"10-K\", ticker, after=f\"{year}-01-01\", before=f\"{year}-12-31\")\n",
    "\n",
    "# Run the download function\n",
    "download_10k(tickers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This created a folder called sec_filings in my local and saved the data. Let us try to preview the contents of some 10-K filings for Apple to confirm that the data is downloaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SEC-DOCUMENT>0000320193-17-000070.txt : 20171103\n",
      "<SEC-HEADER>0000320193-17-000070.hdr.sgml : 20171103\n",
      "<ACCEPTANCE-DATETIME>20171103080137\n",
      "ACCESSION NUMBER:\t\t0000320193-17-000070\n",
      "CONFORMED SUBMISSION TYPE:\t10-K\n",
      "PUBLIC DOCUMENT COUNT:\t\t97\n",
      "CONFORMED PERIOD OF REPORT:\t20170930\n",
      "FILED AS OF DATE:\t\t20171103\n",
      "DATE AS OF CHANGE:\t\t20171103\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def preview_file(file_path, lines=10):\n",
    "    \"\"\" Print the first 'lines' lines of the file located at 'file_path' \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for _ in range(lines):\n",
    "            print(file.readline().strip())\n",
    "\n",
    "# Path to a specific 10-K file\n",
    "file_path = os.path.join(os.getcwd(), \"sec_filings\", \"AAPL\", \"10-K\", \"D:/sec-edgar-filings/AAPL/10-K/0000320193-17-000070/full-submission.txt\")\n",
    "preview_file(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further perform analysis and generate meaningful visualizations, let us merge the data from 3 different companies and store it in a pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for files in D:/sec-edgar-filings/\n",
      "No files found in D:/sec-edgar-filings/\n",
      "No files found in D:/sec-edgar-filings/AAPL\n",
      "No files found in D:/sec-edgar-filings/AAPL\\10-K\n",
      "Processing file: D:/sec-edgar-filings/AAPL\\10-K\\0000320193-17-000070\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/AAPL\\10-K\\0000320193-18-000145\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/AAPL\\10-K\\0000320193-19-000119\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/AAPL\\10-K\\0000320193-20-000096\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/AAPL\\10-K\\0000320193-21-000105\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/AAPL\\10-K\\0000320193-22-000108\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/AAPL\\10-K\\0000320193-23-000106\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/AAPL\\10-K\\0000320193-95-000016\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/AAPL\\10-K\\0000320193-96-000023\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/AAPL\\10-K\\0000912057-00-053623\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/AAPL\\10-K\\0000912057-99-010244\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/AAPL\\10-K\\0001047469-02-007674\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/AAPL\\10-K\\0001047469-03-041604\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/AAPL\\10-K\\0001047469-04-035975\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/AAPL\\10-K\\0001047469-07-009340\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/AAPL\\10-K\\0001047469-97-006960\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/AAPL\\10-K\\0001104659-05-058421\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/AAPL\\10-K\\0001104659-06-084288\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/AAPL\\10-K\\0001193125-08-224958\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/AAPL\\10-K\\0001193125-09-214859\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/AAPL\\10-K\\0001193125-10-238044\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/AAPL\\10-K\\0001193125-11-282113\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/AAPL\\10-K\\0001193125-12-444068\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/AAPL\\10-K\\0001193125-13-416534\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/AAPL\\10-K\\0001193125-14-383437\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/AAPL\\10-K\\0001193125-15-356351\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/AAPL\\10-K\\0001628280-16-020309\\full-submission.txt\n",
      "No files found in D:/sec-edgar-filings/GOOGL\n",
      "No files found in D:/sec-edgar-filings/GOOGL\\10-K\n",
      "Processing file: D:/sec-edgar-filings/GOOGL\\10-K\\0001652044-16-000012\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/GOOGL\\10-K\\0001652044-17-000008\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/GOOGL\\10-K\\0001652044-18-000007\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/GOOGL\\10-K\\0001652044-19-000004\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/GOOGL\\10-K\\0001652044-20-000008\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/GOOGL\\10-K\\0001652044-21-000010\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/GOOGL\\10-K\\0001652044-22-000019\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/GOOGL\\10-K\\0001652044-23-000016\\full-submission.txt\n",
      "No files found in D:/sec-edgar-filings/MSFT\n",
      "No files found in D:/sec-edgar-filings/MSFT\\10-K\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0000891020-95-000433\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0000891020-96-001130\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0000950170-23-035122\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0001017062-97-001764\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0001032210-00-001961\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0001032210-01-501099\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0001032210-02-001351\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0001032210-98-001067\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0001032210-99-001375\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0001193125-03-045632\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0001193125-04-150689\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0001193125-05-174825\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0001193125-06-180008\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0001193125-07-170817\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0001193125-08-162768\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0001193125-09-158735\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0001193125-10-171791\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0001193125-11-200680\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0001193125-12-316848\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0001193125-13-310206\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0001193125-14-289961\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0001193125-15-272806\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0001193125-16-662209\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0001564590-17-014900\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0001564590-18-019062\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0001564590-19-027952\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0001564590-20-034944\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0001564590-21-039151\\full-submission.txt\n",
      "Processing file: D:/sec-edgar-filings/MSFT\\10-K\\0001564590-22-026876\\full-submission.txt\n",
      "DataFrame created with 64 records.\n"
     ]
    }
   ],
   "source": [
    "# Set the correct directory\n",
    "download_dir = \"D:/sec-edgar-filings/\"\n",
    "\n",
    "def read_and_merge_files(directory):\n",
    "    data_frames = []\n",
    "    print(f\"Looking for files in {directory}\")\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        if not files:\n",
    "            print(f\"No files found in {root}\")\n",
    "            continue\n",
    "        for file in files:\n",
    "            path = os.path.join(root, file)\n",
    "            print(f\"Processing file: {path}\")\n",
    "            try:\n",
    "                with open(path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "                # The company name is extracted by going up two directories\n",
    "                company = os.path.basename(os.path.dirname(os.path.dirname(root)))\n",
    "                # Extract the year component from the filing folder name\n",
    "                partial_year = os.path.basename(root).split('-')[1]  # Assumes year is second part of the hyphenated string\n",
    "                # Determine the correct century prefix\n",
    "                if partial_year.isdigit():  # Ensures the extracted part is numerical\n",
    "                    year = int(partial_year)\n",
    "                    if year < 95:\n",
    "                        full_year = 2000 + year\n",
    "                    else:\n",
    "                        full_year = 1900 + year\n",
    "                else:\n",
    "                    full_year = partial_year  # Use the full year if available\n",
    "                data_frames.append(pd.DataFrame({'Company': [company], 'Filing Year': [full_year], 'Text': [text]}))\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to read file {file}: {e}\")\n",
    "    if data_frames:\n",
    "        return pd.concat(data_frames, ignore_index=True)\n",
    "    else:\n",
    "        print(\"No data to concatenate. Returning empty DataFrame.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Use the function with the correct path\n",
    "df = read_and_merge_files(download_dir)\n",
    "\n",
    "if df.empty:\n",
    "    print(\"DataFrame is empty.\")\n",
    "else:\n",
    "    print(f\"DataFrame created with {len(df)} records.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracted the Company name, Year and the Text in 3 different columns.\n",
    "\n",
    "The folder name 0000320193-18-000145 is a SEC filing identifier. This identifier is a unique sequence used by the Securities and Exchange Commission (SEC) to label specific filings. Here’s a breakdown of what the components typically represent:\n",
    "\n",
    "CIK (Central Index Key): 0000320193 is likely the CIK, which is a unique number assigned to all companies who file with the SEC. This particular CIK corresponds to Apple Inc.\n",
    "Year: 18 suggests the filing was made in 2018.\n",
    "Sequence Number: 000145 is a unique sequence number that identifies this specific filing among all filings made by the company in that year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the first 5 records of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Filing Year</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2017</td>\n",
       "      <td>&lt;SEC-DOCUMENT&gt;0000320193-17-000070.txt : 20171...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2018</td>\n",
       "      <td>&lt;SEC-DOCUMENT&gt;0000320193-18-000145.txt : 20181...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2019</td>\n",
       "      <td>&lt;SEC-DOCUMENT&gt;0000320193-19-000119.txt : 20191...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020</td>\n",
       "      <td>&lt;SEC-DOCUMENT&gt;0000320193-20-000096.txt : 20201...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2021</td>\n",
       "      <td>&lt;SEC-DOCUMENT&gt;0000320193-21-000105.txt : 20211...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Company  Filing Year                                               Text\n",
       "0    AAPL         2017  <SEC-DOCUMENT>0000320193-17-000070.txt : 20171...\n",
       "1    AAPL         2018  <SEC-DOCUMENT>0000320193-18-000145.txt : 20181...\n",
       "2    AAPL         2019  <SEC-DOCUMENT>0000320193-19-000119.txt : 20191...\n",
       "3    AAPL         2020  <SEC-DOCUMENT>0000320193-20-000096.txt : 20201...\n",
       "4    AAPL         2021  <SEC-DOCUMENT>0000320193-21-000105.txt : 20211..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 Text  \\\n",
      "0   <SEC-DOCUMENT>0000320193-17-000070.txt : 20171...   \n",
      "1   <SEC-DOCUMENT>0000320193-18-000145.txt : 20181...   \n",
      "2   <SEC-DOCUMENT>0000320193-19-000119.txt : 20191...   \n",
      "3   <SEC-DOCUMENT>0000320193-20-000096.txt : 20201...   \n",
      "4   <SEC-DOCUMENT>0000320193-21-000105.txt : 20211...   \n",
      "..                                                ...   \n",
      "59  <SEC-DOCUMENT>0001564590-18-019062.txt : 20180...   \n",
      "60  <SEC-DOCUMENT>0001564590-19-027952.txt : 20190...   \n",
      "61  <SEC-DOCUMENT>0001564590-20-034944.txt : 20200...   \n",
      "62  <SEC-DOCUMENT>0001564590-21-039151.txt : 20210...   \n",
      "63  <SEC-DOCUMENT>0001564590-22-026876.txt : 20220...   \n",
      "\n",
      "                                         Cleaned Text  \n",
      "0   0000320193-17-000070.txt : 20171103 0000320193...  \n",
      "1   0000320193-18-000145.txt : 20181105 0000320193...  \n",
      "2   0000320193-19-000119.txt : 20191031 0000320193...  \n",
      "3   0000320193-20-000096.txt : 20201030 0000320193...  \n",
      "4   0000320193-21-000105.txt : 20211029 0000320193...  \n",
      "..                                                ...  \n",
      "59  0001564590-18-019062.txt : 20180803 0001564590...  \n",
      "60  0001564590-19-027952.txt : 20190801 0001564590...  \n",
      "61  0001564590-20-034944.txt : 20200918 0001564590...  \n",
      "62  0001564590-21-039151.txt : 20210729 0001564590...  \n",
      "63  0001564590-22-026876.txt : 20220728 0001564590...  \n",
      "\n",
      "[64 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "def extract_and_clean_text(xml_content):\n",
    "    try:\n",
    "        # Parse using lxml which can handle more complex and broken XML structures\n",
    "        parser = etree.XMLParser(recover=True)  # recover from errors\n",
    "        tree = etree.parse(io.StringIO(xml_content), parser)\n",
    "        root = tree.getroot()\n",
    "        if root is None:\n",
    "            return \"XML Parsing Error: No root element found\"\n",
    "        return extract_text_from_xml(root)\n",
    "    except etree.XMLSyntaxError as e:\n",
    "        return f\"XML Parsing Error: {str(e)}\"\n",
    "    except Exception as e:\n",
    "        return f\"Unexpected Error: {str(e)}\"\n",
    "\n",
    "def extract_text_from_xml(element):\n",
    "    text_parts = []\n",
    "    if element is not None:\n",
    "        if element.text:\n",
    "            text_parts.append(element.text.strip())\n",
    "        for child in element:\n",
    "            text_parts.append(extract_text_from_xml(child))\n",
    "        if element.tail:\n",
    "            text_parts.append(element.tail.strip())\n",
    "    return ' '.join(filter(None, text_parts))\n",
    "\n",
    "# Assuming 'df' is your existing DataFrame and it has a column named 'Text'\n",
    "# containing the XML and HTML content.\n",
    "\n",
    "# Apply the extraction and cleaning process to each entry in the 'Text' column\n",
    "df['Cleaned Text'] = df['Text'].apply(extract_and_clean_text)\n",
    "\n",
    "# Print the DataFrame to see the original and cleaned text\n",
    "print(df[['Text', 'Cleaned Text']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the cleaned data to a local directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where text files will be stored\n",
    "output_dir = 'output_texts'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Iterate over the DataFrame rows\n",
    "for index, row in df.iterrows():\n",
    "    # Create a file path\n",
    "    file_path = os.path.join(output_dir, f\"text_{index}.txt\")\n",
    "    # Write text to a file\n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(str(row['Cleaned Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text_0.txt and saved to D:/cleaned_output_texts\\text_0.txt\n",
      "Cleaned text_1.txt and saved to D:/cleaned_output_texts\\text_1.txt\n",
      "Cleaned text_10.txt and saved to D:/cleaned_output_texts\\text_10.txt\n",
      "Cleaned text_11.txt and saved to D:/cleaned_output_texts\\text_11.txt\n",
      "Cleaned text_12.txt and saved to D:/cleaned_output_texts\\text_12.txt\n",
      "Cleaned text_13.txt and saved to D:/cleaned_output_texts\\text_13.txt\n",
      "Cleaned text_14.txt and saved to D:/cleaned_output_texts\\text_14.txt\n",
      "Cleaned text_15.txt and saved to D:/cleaned_output_texts\\text_15.txt\n",
      "Cleaned text_16.txt and saved to D:/cleaned_output_texts\\text_16.txt\n",
      "Cleaned text_17.txt and saved to D:/cleaned_output_texts\\text_17.txt\n",
      "Cleaned text_18.txt and saved to D:/cleaned_output_texts\\text_18.txt\n",
      "Cleaned text_19.txt and saved to D:/cleaned_output_texts\\text_19.txt\n",
      "Cleaned text_2.txt and saved to D:/cleaned_output_texts\\text_2.txt\n",
      "Cleaned text_20.txt and saved to D:/cleaned_output_texts\\text_20.txt\n",
      "Cleaned text_21.txt and saved to D:/cleaned_output_texts\\text_21.txt\n",
      "Cleaned text_22.txt and saved to D:/cleaned_output_texts\\text_22.txt\n",
      "Cleaned text_23.txt and saved to D:/cleaned_output_texts\\text_23.txt\n",
      "Cleaned text_24.txt and saved to D:/cleaned_output_texts\\text_24.txt\n",
      "Cleaned text_25.txt and saved to D:/cleaned_output_texts\\text_25.txt\n",
      "Cleaned text_26.txt and saved to D:/cleaned_output_texts\\text_26.txt\n",
      "Cleaned text_27.txt and saved to D:/cleaned_output_texts\\text_27.txt\n",
      "Cleaned text_28.txt and saved to D:/cleaned_output_texts\\text_28.txt\n",
      "Cleaned text_29.txt and saved to D:/cleaned_output_texts\\text_29.txt\n",
      "Cleaned text_3.txt and saved to D:/cleaned_output_texts\\text_3.txt\n",
      "Cleaned text_30.txt and saved to D:/cleaned_output_texts\\text_30.txt\n",
      "Cleaned text_31.txt and saved to D:/cleaned_output_texts\\text_31.txt\n",
      "Cleaned text_32.txt and saved to D:/cleaned_output_texts\\text_32.txt\n",
      "Cleaned text_33.txt and saved to D:/cleaned_output_texts\\text_33.txt\n",
      "Cleaned text_34.txt and saved to D:/cleaned_output_texts\\text_34.txt\n",
      "Cleaned text_35.txt and saved to D:/cleaned_output_texts\\text_35.txt\n",
      "Cleaned text_36.txt and saved to D:/cleaned_output_texts\\text_36.txt\n",
      "Cleaned text_37.txt and saved to D:/cleaned_output_texts\\text_37.txt\n",
      "Cleaned text_38.txt and saved to D:/cleaned_output_texts\\text_38.txt\n",
      "Cleaned text_39.txt and saved to D:/cleaned_output_texts\\text_39.txt\n",
      "Cleaned text_4.txt and saved to D:/cleaned_output_texts\\text_4.txt\n",
      "Cleaned text_40.txt and saved to D:/cleaned_output_texts\\text_40.txt\n",
      "Cleaned text_41.txt and saved to D:/cleaned_output_texts\\text_41.txt\n",
      "Cleaned text_42.txt and saved to D:/cleaned_output_texts\\text_42.txt\n",
      "Cleaned text_43.txt and saved to D:/cleaned_output_texts\\text_43.txt\n",
      "Cleaned text_44.txt and saved to D:/cleaned_output_texts\\text_44.txt\n",
      "Cleaned text_45.txt and saved to D:/cleaned_output_texts\\text_45.txt\n",
      "Cleaned text_46.txt and saved to D:/cleaned_output_texts\\text_46.txt\n",
      "Cleaned text_47.txt and saved to D:/cleaned_output_texts\\text_47.txt\n",
      "Cleaned text_48.txt and saved to D:/cleaned_output_texts\\text_48.txt\n",
      "Cleaned text_49.txt and saved to D:/cleaned_output_texts\\text_49.txt\n",
      "Cleaned text_5.txt and saved to D:/cleaned_output_texts\\text_5.txt\n",
      "Cleaned text_50.txt and saved to D:/cleaned_output_texts\\text_50.txt\n",
      "Cleaned text_51.txt and saved to D:/cleaned_output_texts\\text_51.txt\n",
      "Cleaned text_52.txt and saved to D:/cleaned_output_texts\\text_52.txt\n",
      "Cleaned text_53.txt and saved to D:/cleaned_output_texts\\text_53.txt\n",
      "Cleaned text_54.txt and saved to D:/cleaned_output_texts\\text_54.txt\n",
      "Cleaned text_55.txt and saved to D:/cleaned_output_texts\\text_55.txt\n",
      "Cleaned text_56.txt and saved to D:/cleaned_output_texts\\text_56.txt\n",
      "Cleaned text_57.txt and saved to D:/cleaned_output_texts\\text_57.txt\n",
      "Cleaned text_58.txt and saved to D:/cleaned_output_texts\\text_58.txt\n",
      "Cleaned text_59.txt and saved to D:/cleaned_output_texts\\text_59.txt\n",
      "Cleaned text_6.txt and saved to D:/cleaned_output_texts\\text_6.txt\n",
      "Cleaned text_60.txt and saved to D:/cleaned_output_texts\\text_60.txt\n",
      "Cleaned text_61.txt and saved to D:/cleaned_output_texts\\text_61.txt\n",
      "Cleaned text_62.txt and saved to D:/cleaned_output_texts\\text_62.txt\n",
      "Cleaned text_63.txt and saved to D:/cleaned_output_texts\\text_63.txt\n",
      "Cleaned text_7.txt and saved to D:/cleaned_output_texts\\text_7.txt\n",
      "Cleaned text_8.txt and saved to D:/cleaned_output_texts\\text_8.txt\n",
      "Cleaned text_9.txt and saved to D:/cleaned_output_texts\\text_9.txt\n"
     ]
    }
   ],
   "source": [
    "def clean_financial_text(input_file):\n",
    "    # Read the entire file\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        text_data = file.read()\n",
    "\n",
    "    # Remove HTML tags\n",
    "    clean_text = re.sub('<.*?>', '', text_data)\n",
    "\n",
    "    # Replace specific patterns with \"Yes\" or \"No\"\n",
    "    clean_text = re.sub(r'Yes ☒ No ☐', ':Yes.', clean_text)\n",
    "    clean_text = re.sub(r'Yes ☐ No ☒', ':No.', clean_text)\n",
    "\n",
    "    # Replace multiple newlines with a single newline\n",
    "    clean_text = re.sub(r'\\n\\s*\\n', '\\n', clean_text)\n",
    "\n",
    "    # Remove all types of brackets and their contents\n",
    "    clean_text = re.sub(r'\\[.*?\\]|\\{.*?}|\\(.*?\\)', '', clean_text)\n",
    "\n",
    "    # Remove unnecessary white spaces\n",
    "    clean_text = re.sub(' +', ' ', clean_text.strip())\n",
    "\n",
    "    return clean_text\n",
    "\n",
    "# Directory containing the text files\n",
    "input_dir = 'D:/output_texts'\n",
    "output_dir = 'D:/cleaned_output_texts'\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Process each file in the directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    file_path = os.path.join(input_dir, filename)\n",
    "    if filename.endswith('.txt'):\n",
    "        cleaned_text = clean_financial_text(file_path)\n",
    "        # Create a path for the cleaned file in the output directory\n",
    "        output_file_path = os.path.join(output_dir, filename)\n",
    "        # Save the cleaned text\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            output_file.write(cleaned_text)\n",
    "        print(f\"Cleaned {filename} and saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_financial_text(text_data):\n",
    "    # Remove HTML tags\n",
    "    clean_text = re.sub('<.*?>', '', text_data)\n",
    "    # Replace specific patterns with \"Yes\" or \"No\"\n",
    "    clean_text = re.sub(r'Yes ☒ No ☐', ':Yes.', clean_text)\n",
    "    clean_text = re.sub(r'Yes ☐ No ☒', ':No.', clean_text)\n",
    "    # Replace multiple newlines with a single newline\n",
    "    clean_text = re.sub(r'\\n\\s*\\n', '\\n', clean_text)\n",
    "    # Remove all types of brackets and their contents\n",
    "    clean_text = re.sub(r'\\[.*?\\]|\\{.*?}|\\(.*?\\)', '', clean_text)\n",
    "    # Remove unnecessary white spaces\n",
    "    clean_text = re.sub(' +', ' ', clean_text.strip())\n",
    "    return clean_text\n",
    "\n",
    "# Assuming df is your existing DataFrame\n",
    "# Apply the cleaning function to the 'Text' column\n",
    "df['Updated Text'] = df['Cleaned Text'].apply(clean_financial_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Filing Year</th>\n",
       "      <th>Text</th>\n",
       "      <th>Cleaned Text</th>\n",
       "      <th>Updated Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2017</td>\n",
       "      <td>&lt;SEC-DOCUMENT&gt;0000320193-17-000070.txt : 20171...</td>\n",
       "      <td>0000320193-17-000070.txt : 20171103 0000320193...</td>\n",
       "      <td>0000320193-17-000070.txt : 20171103 0000320193...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2018</td>\n",
       "      <td>&lt;SEC-DOCUMENT&gt;0000320193-18-000145.txt : 20181...</td>\n",
       "      <td>0000320193-18-000145.txt : 20181105 0000320193...</td>\n",
       "      <td>0000320193-18-000145.txt : 20181105 0000320193...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2019</td>\n",
       "      <td>&lt;SEC-DOCUMENT&gt;0000320193-19-000119.txt : 20191...</td>\n",
       "      <td>0000320193-19-000119.txt : 20191031 0000320193...</td>\n",
       "      <td>0000320193-19-000119.txt : 20191031 0000320193...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2020</td>\n",
       "      <td>&lt;SEC-DOCUMENT&gt;0000320193-20-000096.txt : 20201...</td>\n",
       "      <td>0000320193-20-000096.txt : 20201030 0000320193...</td>\n",
       "      <td>0000320193-20-000096.txt : 20201030 0000320193...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>2021</td>\n",
       "      <td>&lt;SEC-DOCUMENT&gt;0000320193-21-000105.txt : 20211...</td>\n",
       "      <td>0000320193-21-000105.txt : 20211029 0000320193...</td>\n",
       "      <td>0000320193-21-000105.txt : 20211029 0000320193...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Company  Filing Year                                               Text  \\\n",
       "0    AAPL         2017  <SEC-DOCUMENT>0000320193-17-000070.txt : 20171...   \n",
       "1    AAPL         2018  <SEC-DOCUMENT>0000320193-18-000145.txt : 20181...   \n",
       "2    AAPL         2019  <SEC-DOCUMENT>0000320193-19-000119.txt : 20191...   \n",
       "3    AAPL         2020  <SEC-DOCUMENT>0000320193-20-000096.txt : 20201...   \n",
       "4    AAPL         2021  <SEC-DOCUMENT>0000320193-21-000105.txt : 20211...   \n",
       "\n",
       "                                        Cleaned Text  \\\n",
       "0  0000320193-17-000070.txt : 20171103 0000320193...   \n",
       "1  0000320193-18-000145.txt : 20181105 0000320193...   \n",
       "2  0000320193-19-000119.txt : 20191031 0000320193...   \n",
       "3  0000320193-20-000096.txt : 20201030 0000320193...   \n",
       "4  0000320193-21-000105.txt : 20211029 0000320193...   \n",
       "\n",
       "                                        Updated Text  \n",
       "0  0000320193-17-000070.txt : 20171103 0000320193...  \n",
       "1  0000320193-18-000145.txt : 20181105 0000320193...  \n",
       "2  0000320193-19-000119.txt : 20191031 0000320193...  \n",
       "3  0000320193-20-000096.txt : 20201030 0000320193...  \n",
       "4  0000320193-21-000105.txt : 20211029 0000320193...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64 entries, 0 to 63\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   Company       64 non-null     object\n",
      " 1   Filing Year   64 non-null     int64 \n",
      " 2   Text          64 non-null     object\n",
      " 3   Cleaned Text  64 non-null     object\n",
      " 4   Updated Text  64 non-null     object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 2.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Company  Filing Year                                               Text  \\\n",
      "3    AAPL         2020  <SEC-DOCUMENT>0000320193-20-000096.txt : 20201...   \n",
      "\n",
      "                                        Cleaned Text  \\\n",
      "3  0000320193-20-000096.txt : 20201030 0000320193...   \n",
      "\n",
      "                                        Updated Text  \n",
      "3  0000320193-20-000096.txt : 20201030 0000320193...  \n"
     ]
    }
   ],
   "source": [
    "print(df[(df['Company'] == \"AAPL\") & (df['Filing Year'] == 2020)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing data for AAPL from 2020:\n",
      "Question: What was the total revenue of Apple in the fiscal year 2020?\n",
      "Answer: $14.3 billion\n",
      "\n",
      "Question: What are the key competitive factors for Apple according to the report?\n",
      "Answer: increased competition\n",
      "\n",
      "Question: What risk factors does Apple face according to their 10-K report?\n",
      "Answer: risks and uncertainties\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Set your API key\n",
    "API_URL = \"https://api-inference.huggingface.co/models/consciousAI/question-answering-roberta-base-s-v2\"\n",
    "headers = {\"Authorization\": \"Bearer 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\"}\n",
    "\n",
    "# Specify the target company and year\n",
    "target_company = \"AAPL\"\n",
    "target_year = 2020\n",
    "\n",
    "# Filter the DataFrame to get the relevant text\n",
    "filtered_df = df[(df['Company'] == target_company) & (df['Filing Year'] == target_year)]\n",
    "\n",
    "if not filtered_df.empty:\n",
    "    text_data = filtered_df.iloc[0]['Updated Text']  # Assuming there's only one match\n",
    "    print(f\"Processing data for {target_company} from {target_year}:\")\n",
    "    \n",
    "    # Define questions\n",
    "    questions = [\n",
    "        \"What was the total revenue of Apple in the fiscal year 2020?\",\n",
    "        \"What are the key competitive factors for Apple according to the report?\",\n",
    "        \"What risk factors does Apple face according to their 10-K report?\"\n",
    "    ]\n",
    "\n",
    "    # Iterate through questions and get answers using the Hugging Face model\n",
    "    for question in questions:\n",
    "        payload = json.dumps({\n",
    "            \"inputs\": {\n",
    "                \"question\": question,\n",
    "                \"context\": text_data\n",
    "            }\n",
    "        })\n",
    "        response = requests.post(API_URL, headers=headers, data=payload)\n",
    "        response_data = response.json()\n",
    "        \n",
    "        # Check if 'answer' key is in the response\n",
    "        if 'answer' in response_data:\n",
    "            print(f\"Question: {question}\\nAnswer: {response_data['answer']}\\n\")\n",
    "        else:\n",
    "            # Print error or entire response if 'answer' key is not present\n",
    "            print(\"Error or unexpected response:\", response_data)\n",
    "else:\n",
    "    print(f\"No data found for {target_company} in {target_year}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the SEC document, we can focus on the sections individually, like the business summary, financial data, or risk factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis Goals\n",
    "Determine what insights or information you are interested in extracting. For example:\n",
    "\n",
    "Sentiment Analysis: Assess the sentiment of the text regarding Microsoft's market outlook or financial health.\n",
    "Entity Recognition: Extract names of companies, places, or dates mentioned in the text.\n",
    "Summarization: Get a concise summary of the entire document or specific sections.\n",
    "Keyword Extraction: Identify key terms that frequently appear in the document to understand focus areas.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "This analysis can help stakeholders understand key aspects of the company's health and strategy without needing to parse through the entire document manually. For investors or analysts, this can save time and provide enhanced decision-making tools based on the extracted data trends.\n",
    "\n",
    "This approach allows you to leverage the power of modern NLP tools to efficiently process and analyze large volumes of text data, turning unstructured text into actionable insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Hugging Face's API for Sentiment Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
